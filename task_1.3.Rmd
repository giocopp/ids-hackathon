---
title: |
  Hackathon 2023 \
  Task 1.3
author: |
  Author: Armande Aboudrar-Meda \
  Group: Armande Aboudrar-Meda, Giorgio Coppola, Giulia Petrilli, Luca Vellage
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: show
    df_print: paged
    highlight: tango
    number_sections: FALSE
    theme: lumen
    toc: TRUE
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
pacman::p_load(tidyverse, dplyr, ggplot2, haven, ggridges, lubridate, hms, RColorBrewer) #loading the package we will use 
```

### Getting first logged residence for all the participants 

```{r}
# Three datasets 
netflix <- readRDS("./tracking/meof_usa_web_df.RDS")
state_residence <- readRDS("./survey/meof_usa_survey_df.RDS")
state_residence <- haven::as_factor(state_residence) # to avoid to do a mapping of the variable inputstate
time_zones <- readRDS("./tracking/time_zones.RDS")

# The main variable we are interested in in the renamed "state_residence" dataset is "inputstate" and we will filter the dataset to have only the first logged state residence which doesn't mean only the first wave ! 
# The main variable we are interested in in the renamed "netflix" dataset is the domain, that we will filter for all the strings containing the word "netflix" so we don't lose information by only taking "netflix.com"

# Filtering dataset to have the first logged state residence
survey_data <- state_residence %>%
  select(wave, personid, inputstate, starttime) %>%
  filter(!is.na(inputstate)) %>%  # dealing with the many NA in inputstate 
  distinct(personid, .keep_all = TRUE) %>% # Remove the duplicates of state residence, because each person id should have only one first logged state_residence
  arrange(personid, starttime) %>%
  group_by(personid) %>%
  slice(1) %>%
  ungroup() %>%
  select(personid, inputstate) # Keeping the variables that we are interested in in the survey_data : personid which will be our key variable to merge the datasets and input state which is the variable telling us about the state of residence in the survey 

# Filtering for the domain netflix 
netflix_data <- netflix %>%
  filter(str_detect(domain, "netflix")) # takes all the domains in which we find the word Netflix 

# Doing a left join to join the data we will use on the use of netflix and the data about the state of residence 
merged_data <- left_join(netflix_data, survey_data, by = "personid") 

# We did the first part of the question ! 
```

### Find the local Times

We will now want to report the data and users’ local times, which requires  to update the time of each web activity based on the user‘s home state and the respective time zone.

```{r pressure, echo=FALSE}
merged_data <- merged_data %>%
  rename(state = inputstate) # Renaming the variable inputstate by state before 

# Final dataset with time-zone associated to the state of residence
final_dataset_timezone <- left_join(merged_data, time_zones, by = "state") %>%
  mutate(
    time_zone = ifelse(is.na(time_zone), "UTC", time_zone) #if NA put UTC 
  )

# Updating the time of each web activity based on the user's home state and the respective time zone
# Another way to do it could have been to subset the final dataset by time_zone and to run the two functions of lubridate : ymd_hms and with_tz on each time zone as we have seen in the workshop on lubridate
final_dataset_timezone$local_time <- mapply(
  function(timestamp, zone) {
    format(with_tz(as.POSIXct(timestamp), tzone = zone), "%Y-%m-%d %H:%M:%S")
  }, 
  final_dataset_timezone$used_at, 
  final_dataset_timezone$time_zone
)
```

### Illustrating with a Ridge Plot 

```{r}
# Data in the format we need to do the ridge plot 
final_dataset_timezone$local_time <- as.POSIXct(final_dataset_timezone$local_time, format = "%Y-%m-%d %H:%M:%S")

# Extracting (minute and second otherwise R rounds it a lot, but actually not necessary because we will not use it after) 
final_dataset_timezone <- final_dataset_timezone %>%
  mutate(
    weekday = factor(weekdays(local_time),
                     levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    hour = lubridate::hour(local_time), 
    minute = lubridate::minute(local_time), 
    second = lubridate::second(local_time) 
  )

activity_by_time_interval <- final_dataset_timezone %>%
  group_by(weekday, hour)
ridge_plot <- ggplot(activity_by_time_interval, aes(x = hour, y = weekday)) +
  geom_density_ridges_gradient(
    aes(fill = as.factor(..quantile..)),
    alpha = 0.3, quantile_lines = TRUE, calc_ecdf = TRUE
  ) +
  scale_fill_brewer(palette = "BuPu") +
  theme_ridges() +
  scale_x_continuous(breaks = c(3,6,9,12,15,18,21)) + 
  labs(
    title = "Distribution of Netflix activity throughout the day by weekday",
    subtitle = "The density represents the distribution of Netflix activity throughout the day by weekday. The three vertical bars are the quartiles",
    x = "Hour of the Day",
    y = "Weekday",
    fill = "Quartile" 
  ) +
  theme_minimal() + 
  theme(
    plot.subtitle = element_text(size = 7) 
  )

ridge_plot
```

